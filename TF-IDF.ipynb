{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889e5fba",
   "metadata": {},
   "source": [
    "**TF-IDF**\n",
    "\n",
    "1. TF (Term Frequency): How often a word appears in a document.\n",
    "\n",
    "2. IDF (Inverse Document Frequency): How unique or rare a word is across all documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816f653",
   "metadata": {},
   "source": [
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection. Itâ€™s a sparse vector representation and can be used as a basic form of word/document embedding, but it lacks semantic understanding compared to neural embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a61df",
   "metadata": {},
   "source": [
    "**The TF-IDF score for a word increases if**\n",
    "\n",
    "* It appears often in a document (high TF).\n",
    "\n",
    "* But is rare across the whole corpus (high IDF).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000f09f",
   "metadata": {},
   "source": [
    "### ðŸ”¹ How is TF-IDF calculated?\n",
    "\n",
    "For a word `w` in a document `d` from a set of documents `D`:\n",
    "\n",
    "**Term Frequency (TF)**  \n",
    "The frequency of word `w` in document `d`:\n",
    "\n",
    "$$\n",
    "TF(w, d) = \\frac{\\text{Number of times } w \\text{ appears in } d}{\\text{Total words in } d}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF)**  \n",
    "The uniqueness of word `w` across all documents `D`:\n",
    "\n",
    "$$\n",
    "IDF(w, D) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents with } w}\\right)\n",
    "$$\n",
    "\n",
    "**TF-IDF Score**  \n",
    "Combining TF and IDF:\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(w, d, D) = TF(w, d) \\times IDF(w, D)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086ded9",
   "metadata": {},
   "source": [
    "**TF-IDF as Word Embedding**\n",
    "\n",
    "* Unlike neural embeddings (like Word2Vec, GloVe, or fastText), TF-IDF is a sparse representation where:\n",
    "\n",
    "* Each word is represented as a vector in the vocabulary space.\n",
    "\n",
    "* The vector length = total number of words in the vocabulary.\n",
    "\n",
    "* Each dimension has a TF-IDF score or 0.\n",
    "\n",
    "* So for a document, you get a vector of shape:\n",
    "[vocab_size], where each entry corresponds to a word's importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ceb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This this this  is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "dense_matrix = X.todense()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0317cb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.31817034, 0.39300367, 0.2601251 , 0.        ,\n",
       "        0.        , 0.2601251 , 0.        , 0.7803753 ],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790520d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.318170  0.393004  0.260125  0.000000  0.000000  0.260125   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.780375  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a024594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2f9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eba2443",
   "metadata": {},
   "source": [
    "**Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0ec42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'document', 'document.', 'document?', 'first', 'is', 'one.', 'second', 'the', 'third', 'this']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "Doc 1: [0.0, 0.0, 0.184, 0.0, 0.184, 0.111, 0.0, 0.0, 0.111, 0.0, 0.333]\n",
      "Doc 2: [0.0, 0.282, 0.215, 0.0, 0.0, 0.129, 0.0, 0.282, 0.129, 0.0, 0.129]\n",
      "Doc 3: [0.282, 0.0, 0.0, 0.0, 0.0, 0.129, 0.282, 0.0, 0.129, 0.282, 0.129]\n",
      "Doc 4: [0.0, 0.0, 0.0, 0.339, 0.258, 0.155, 0.0, 0.0, 0.155, 0.0, 0.155]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    'This this this  is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "\n",
    "# Preprocessing: tokenize and lowercase\n",
    "def tokenize(doc):\n",
    "    return doc.lower().split()\n",
    "\n",
    "# Step 1: Build vocabulary\n",
    "vocab = set()\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in documents:\n",
    "    tokens = tokenize(doc)\n",
    "    tokenized_docs.append(tokens)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Step 2: Compute TF\n",
    "def compute_tf(tokens):\n",
    "    tf = Counter(tokens)\n",
    "    total_terms = len(tokens)\n",
    "    return {term: count / total_terms for term, count in tf.items()}\n",
    "\n",
    "# Step 3: Compute IDF\n",
    "def compute_idf(doc_list):\n",
    "    N = len(doc_list)\n",
    "    idf = {}\n",
    "    for term in vocab:\n",
    "        doc_count = sum(term in doc for doc in doc_list)\n",
    "        idf[term] = math.log((N / (1 + doc_count))) + 1  # smoothed IDF\n",
    "    return idf\n",
    "\n",
    "# Step 4: Compute TF-IDF\n",
    "def compute_tfidf(docs):\n",
    "    idf = compute_idf(docs)\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for tokens in docs:\n",
    "        tf = compute_tf(tokens)\n",
    "        tfidf_vector = [tf.get(term, 0) * idf[term] for term in vocab]\n",
    "        tfidf_matrix.append(tfidf_vector)\n",
    "\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Run it\n",
    "tfidf_matrix = compute_tfidf(tokenized_docs)\n",
    "\n",
    "# Display the result\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "for i, vec in enumerate(tfidf_matrix):\n",
    "    print(f\"Doc {i+1}:\", [round(val, 3) for val in vec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed03cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def TFIDF(corpus):\n",
    "\n",
    "    def tokenizer(text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    vocab = []\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        tokens = tokenizer(doc)\n",
    "        tokenized_corpus.append(tokens)\n",
    "        vocab.extend(tokens)\n",
    "\n",
    "    vocab = sorted(set(vocab))\n",
    "    \n",
    "    # Calculate Term Frequency (TF)\n",
    "    tf_list = []\n",
    "    for tokens in tokenized_corpus:\n",
    "        tf = {word: 0 for word in vocab}\n",
    "        for word in tokens:\n",
    "            tf[word] += 1\n",
    "        total_terms = len(tokens)\n",
    "        for word in tf:\n",
    "            tf[word] = tf[word] / total_terms\n",
    "        tf_list.append(tf)\n",
    "\n",
    "    # Calculate Document Frequency (DF)\n",
    "    df = {word: 0 for word in vocab}\n",
    "    for word in vocab:\n",
    "        for tokens in tokenized_corpus:\n",
    "            if word in tokens:\n",
    "                df[word] += 1\n",
    "\n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    N = len(corpus)\n",
    "    idf = {word: math.log(N / df[word]) for word in vocab}\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tfidf_list = []\n",
    "    for tf in tf_list:\n",
    "        tfidf = {word: tf[word] * idf[word] for word in vocab}\n",
    "        tfidf_list.append(tfidf)\n",
    "\n",
    "    df_tfidf = pd.DataFrame(tfidf_list)\n",
    "    return df_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09e1f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and  document  document.  document?     first   is      one.  \\\n",
      "0  0.000000  0.000000   0.099021   0.000000  0.099021  0.0  0.000000   \n",
      "1  0.000000  0.231049   0.115525   0.000000  0.000000  0.0  0.000000   \n",
      "2  0.231049  0.000000   0.000000   0.000000  0.000000  0.0  0.231049   \n",
      "3  0.000000  0.000000   0.000000   0.277259  0.138629  0.0  0.000000   \n",
      "\n",
      "     second  the     third  this  \n",
      "0  0.000000  0.0  0.000000   0.0  \n",
      "1  0.231049  0.0  0.000000   0.0  \n",
      "2  0.000000  0.0  0.231049   0.0  \n",
      "3  0.000000  0.0  0.000000   0.0  \n"
     ]
    }
   ],
   "source": [
    "print(TFIDF(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b1e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This this this  is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5056bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de761355",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "tokenized_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "710c8d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'this', 'this', 'is', 'the', 'first', 'document.']\n",
      "['this', 'document', 'is', 'the', 'second', 'document.']\n",
      "['and', 'this', 'is', 'the', 'third', 'one.']\n",
      "['is', 'this', 'the', 'first', 'document?']\n"
     ]
    }
   ],
   "source": [
    "for i in corpus:\n",
    "    tok = tokenizer(i)\n",
    "    print(tok)\n",
    "    tokenized_corpus.append(tok)\n",
    "    vocab.extend(tok)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd4b514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'this', 'this', 'is', 'the', 'first', 'document.'],\n",
       " ['this', 'document', 'is', 'the', 'second', 'document.'],\n",
       " ['and', 'this', 'is', 'the', 'third', 'one.'],\n",
       " ['is', 'this', 'the', 'first', 'document?']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6fc3352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'this',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'document.',\n",
       " 'this',\n",
       " 'document',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'document.',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'third',\n",
       " 'one.',\n",
       " 'is',\n",
       " 'this',\n",
       " 'the',\n",
       " 'first',\n",
       " 'document?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763485b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'document',\n",
       " 'document.',\n",
       " 'document?',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one.',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484304e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 0, 'document.': 1, 'document?': 0, 'first': 1, 'is': 1, 'one.': 0, 'second': 0, 'the': 1, 'third': 0, 'this': 3}\n",
      "{'and': 0, 'document': 1, 'document.': 1, 'document?': 0, 'first': 0, 'is': 1, 'one.': 0, 'second': 1, 'the': 1, 'third': 0, 'this': 1}\n",
      "{'and': 1, 'document': 0, 'document.': 0, 'document?': 0, 'first': 0, 'is': 1, 'one.': 1, 'second': 0, 'the': 1, 'third': 1, 'this': 1}\n",
      "{'and': 0, 'document': 0, 'document.': 0, 'document?': 1, 'first': 1, 'is': 1, 'one.': 0, 'second': 0, 'the': 1, 'third': 0, 'this': 1}\n"
     ]
    }
   ],
   "source": [
    "tf_list = []\n",
    "for tokens in tokenized_corpus:\n",
    "    tf = {word: 0 for word in vocab}\n",
    "    # print(tf)\n",
    "    for word in tokens:\n",
    "        # print(word)\n",
    "        tf[word] +=1\n",
    "    print(tf)\n",
    "    for word in tf:\n",
    "        tf[word] = tf[word] / len(tokens)\n",
    "    tf_list.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c3d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {word: 0 for word in vocab}\n",
    "for word in vocab:\n",
    "    for tokens in tokenized_corpus:\n",
    "        if word in tokens:\n",
    "            df[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431878df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'document': 1,\n",
       " 'document.': 2,\n",
       " 'document?': 1,\n",
       " 'first': 2,\n",
       " 'is': 4,\n",
       " 'one.': 1,\n",
       " 'second': 1,\n",
       " 'the': 4,\n",
       " 'third': 1,\n",
       " 'this': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d836e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(corpus)\n",
    "idf = {word: math.log(N / df[word]) for word in vocab}\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tfidf_list = []\n",
    "for tf in tf_list:\n",
    "    tfidf = {word: tf[word] * idf[word] for word in vocab}\n",
    "    tfidf_list.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1490aa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>document.</th>\n",
       "      <th>document?</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one.</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277259</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document  document.  document?     first   is      one.  \\\n",
       "0  0.000000  0.000000   0.099021   0.000000  0.099021  0.0  0.000000   \n",
       "1  0.000000  0.231049   0.115525   0.000000  0.000000  0.0  0.000000   \n",
       "2  0.231049  0.000000   0.000000   0.000000  0.000000  0.0  0.231049   \n",
       "3  0.000000  0.000000   0.000000   0.277259  0.138629  0.0  0.000000   \n",
       "\n",
       "     second  the     third  this  \n",
       "0  0.000000  0.0  0.000000   0.0  \n",
       "1  0.231049  0.0  0.000000   0.0  \n",
       "2  0.000000  0.0  0.231049   0.0  \n",
       "3  0.000000  0.0  0.000000   0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(tfidf_list)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8016440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
